# Genomic bottleneck and development

## 1. Why indirect encoding is necessary

Directly evolving all synaptic weights does not scale:

- the search space grows with the number of synapses,
- mutation becomes increasingly destructive,
- and evaluation cost grows with model size.

Biological nervous systems avoid this by encoding **developmental programs**: small genomes generate large circuits with structure and regularity. Computationally, this corresponds to **indirect encodings** and **genomic bottlenecks**.

Shuvaev et al. show that a small generator network can specify a much larger target network while retaining high performance, supporting bottleneck-based inductive bias [@shuvaev2024genomic].

---

## 2. Genotype-to-phenotype decomposition

We represent a genome as a set of small networks (“rules”) that generate a phenotype network.

### 2.1 Neuron identities

Each neuron \(i\) receives an identity embedding \(z_i \in \mathbb{R}^d\):

\[
z_i = \text{CPPN}_\psi(\text{pos}_i, \text{tags}_i)
\]

where \(\text{pos}_i\) includes spatial coordinates (or graph coordinates) and \(\text{tags}_i\) may include functional labels (sensory, interneuron candidate, motor).

The CPPN parameters \(\psi\) are part of the genome.

### 2.2 Connection rule

A connection probability (or mask) is generated by:

\[
p_{ij} = \sigma\left(c_\theta(z_i, z_j)\right)
\]

Connections are sampled (or deterministically thresholded) to produce a sparse adjacency \(A_{ij}\in\{0,1\}\).

### 2.3 Weight rule

Initial synaptic weights are generated by:

\[
w_{ij}(t=0) = A_{ij}\cdot g_\phi(z_i, z_j)
\]

This separates **structural connectivity** (mask) from **weight magnitude** generation.

### 2.4 Plasticity rule

Plasticity coefficients for synapse \(j\rightarrow i\) are generated by:

\[
(A_{ij}, B_{ij}, C_{ij}, D_{ij}, E_{ij}, \eta_{ij}, \lambda_{ij}) = p_\omega(z_i, z_j)
\]

where \(p_\omega\) is a small network producing the local learning-rule parameters.

### 2.5 Modulator role rule

We require the agent to compute modulatory signals internally. The genome provides a role assignment:

\[
r_i = \text{softmax}(m_\kappa(z_i))
\]

where roles include (examples): standard neuron, modulator contributor, sensory relay, motor readout contributor.

In Strain A, this is flexible and does not assign biological semantics like “dopamine neuron”—it only enables the network to *choose* which units contribute to modulatory readouts.

---

## 3. Development algorithm (“compiler”)

Development maps the genome to a concrete `AgentParams` instance.

### 3.1 Deterministic development

To ensure reproducibility and stable evaluation:

- development must be deterministic given \((\text{genome}, \text{dev_config}, \text{rng})\),
- all sampling uses explicit JAX PRNG streams,
- outputs must have static shapes (or a bounded sparse representation).

### 3.2 Sparse representation

We represent connectivity as:

- `edge_index` (E×2 int32)
- `edge_weight` (E×1 float32)
- `edge_plasticity_params` (E×P float32)

This avoids dense \(N\times N\) matrices while enabling JAX-friendly operations.

### 3.3 Pseudocode

```text
develop(genome, dev_cfg, rng):
  z = cppn(genome.cppn, neuron_positions, neuron_tags)      # (N,d)
  logits = connection_rule(genome.conn_rule, z_i, z_j)      # (candidate_edges,)
  A = sample_or_threshold(logits, rng_conn)                # sparse mask

  w0 = weight_rule(genome.weight_rule, z_i, z_j) * A       # edge weights
  plast = plasticity_rule(genome.plast_rule, z_i, z_j)     # edge plasticity coeffs
  roles = role_rule(genome.role_rule, z)                   # neuron roles

  return AgentParams(edges=A, w0=w0, plast=plast, roles=roles, neuron_params=...)
```

### 3.4 Evolvable developmental schedules (maturational constraints)

Development need not only generate *structure* (neurons/synapses). The genome can also encode **developmental programs** that schedule *capabilities over age*, e.g.:

- sensory maturation: staged availability and resolution of observation channels,
- motor maturation: staged action magnitude/type availability,
- plasticity maturation: critical-period-like schedules for baseline learning sensitivity.

This “developmental evolution inside the agent” connects to maturational-constraint ideas in developmental robotics [@law2014longitudinal] and to developmental psychobiology perspectives where early sensory limitations shape what is learnable and how neural organization emerges [@turkewitz1982limitations; @turkewitz1985role; @lickliter2000sensory]. Simulation platforms that explicitly model body and sensory development (e.g., MIMo v2) make these hypotheses testable in multimodal regimes [@lopez2025mimogrows].

Practically, these schedules can be encoded as a small set of genome parameters (e.g., \(\{\text{min}, \text{max}, \phi_{\text{start}}, \phi_{\text{end}}\}\) for each gate), preserving the bottleneck while enabling evolution to discover “when” and “how fast” maturation should occur.

---

## 4. Bottleneck size and scaling

The bottleneck is defined by:

- parameter count of CPPN + rule networks,
- and the output phenotype size (neurons + synapses).

We treat bottleneck strength as a controllable hyperparameter:

- small genomes enforce stronger regularity, improving transfer and robustness but potentially limiting expressivity,
- larger genomes allow more idiosyncratic circuits but expand the search space.

This is an explicit experimental knob: we do not assume “more compression is always better”.

---

## 5. Relationship to known indirect encodings

### 5.1 HyperNEAT and CPPN-based connectivity

HyperNEAT uses CPPNs to generate connectivity patterns on geometric substrates, leveraging regularities [@stanley2009hyperneat]. This general benefit of indirect encodings for regular problems is also discussed in indirect-encoding work on the regularity continuum [@clune2011regularity].

We adopt a similar spirit but generalize:

- separate connection and weight rules,
- explicit plasticity rule generation,
- explicit modulatory role generation.

### 5.2 Genomic bottleneck via hypernetworks

Shuvaev-style genomic bottleneck uses a small generator network that outputs target weights, demonstrating extreme compression with small performance loss [@shuvaev2024genomic].

Our formulation is compatible:

- the weight-rule network \(g_\phi\) can be exactly that generator.

### 5.3 Developmental growth encodings (optional extensions)

Optional future encoding backends (to compare against CPPN rules):

- Neural Cellular Automata growth (HyperNCA-style) [@najarro2022hypernca]
- Analog Genetic Encoding (AGE), a gene-regulatory inspired indirect encoding [@mattiussi2007age]
- Developmental / indirect encoding approaches reviewed in neuroevolution surveys [@stanley2019neuroevolution]

These are not baseline; they are experiments once the CPPN-rule baseline is stable.

---

## 6. Genome mutation and inheritance

Because genomes are small neural networks, we use structured mutation operators:

- Gaussian noise per parameter group (CPPN vs rules vs role nets),
- occasional architectural mutations (optional: NEAT-like historical markings) [@stanley2002neat],
- group-wise learning-rate scaling to preserve delicate rule networks.

A key requirement: mutation should be **less often catastrophic** than direct weight mutation.

---

## 7. Ablations and falsification tests

To keep the thesis claims grounded, we run systematic ablations:

1. **Direct encoding** baseline: evolve weights directly (small networks only).
2. **Bottleneck only**: indirect encoding but fixed plasticity (no learning in life).
3. **Plasticity only**: direct encoding + evolved plasticity (no bottleneck).
4. **Full model**: bottleneck + evolved plasticity + modulatory discovery.

We evaluate:

- scaling with network size,
- robustness to environment variation,
- learning-to-learn metrics.

---

## 8. Deliverables

- A genome specification and parameterization.
- A deterministic development pipeline producing sparse `AgentParams`.
- A set of mutations/inheritance operators.
- A test suite:
  - development determinism,
  - connectivity sanity (bounded degree, no NaNs),
  - phenotype stability under small genome perturbations.
